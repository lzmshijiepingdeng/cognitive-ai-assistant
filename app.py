"""
è®¤çŸ¥å‹ AI æ€ç»´æŒ‘æˆ˜åŠ©æ‰‹

è¿™æ˜¯ä¸€ä¸ªåŸºäº Streamlit å’Œ LangChain çš„è®¤çŸ¥å‹ AI åŠ©æ‰‹åº”ç”¨ï¼Œ
æ—¨åœ¨å¸®åŠ©ç”¨æˆ·æå‡æ‰¹åˆ¤æ€§æ€ç»´èƒ½åŠ›ã€‚

ä½œè€…: AI Assistant
ç‰ˆæœ¬: 1.0.0
"""

import os
import time
from typing import Optional

import streamlit as st
from dotenv import load_dotenv
from langchain.chains import LLMChain
from langchain_community.chat_models import ChatOpenAI, ChatAnthropic
from langchain.prompts import ChatPromptTemplate
from openai import APITimeoutError, APIError

# DeepSeek é›†æˆ
try:
    from langchain_deepseek import ChatDeepSeek
except ImportError:
    # å¦‚æœ langchain-deepseek ä¸å¯ç”¨ï¼Œä½¿ç”¨ OpenAI å…¼å®¹çš„æ–¹å¼
    ChatDeepSeek = None


def load_environment() -> tuple[Optional[str], str]:
    """
    åŠ è½½ç¯å¢ƒå˜é‡å¹¶è·å– API å¯†é’¥
    
    Returns:
        tuple[Optional[str], str]: (API å¯†é’¥, API ç±»å‹)
    """
    # å°è¯•åŠ è½½ .env æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åŠ è½½ openkey.env
    if os.path.exists('.env'):
        load_dotenv('.env')
    else:
        load_dotenv('openkey.env')
    
    api_key = os.getenv("OPENAI_API_KEY")
    
    if not api_key:
        st.error("è¯·å…ˆåœ¨ .env æˆ– openkey.env æ–‡ä»¶ä¸­è®¾ç½®ä½ çš„ API å¯†é’¥")
        st.stop()
    
    # æ£€æŸ¥ API å¯†é’¥æ ¼å¼å¹¶ç¡®å®š API ç±»å‹
    if api_key.startswith("sk-proj-"):
        return api_key, "anthropic"
    elif api_key.startswith("sk-"):
        return api_key, "openai"
    elif api_key.startswith("sk-") and len(api_key) > 50:  # DeepSeek å¯†é’¥é€šå¸¸æ›´é•¿
        return api_key, "deepseek"
    else:
        st.warning("âš ï¸ API å¯†é’¥æ ¼å¼å¯èƒ½ä¸æ­£ç¡®")
        st.info("ğŸ’¡ æ”¯æŒçš„æ ¼å¼ï¼š\n- OpenAI: sk-...\n- Anthropic: sk-proj-...\n- DeepSeek: sk-...")
        return api_key, "unknown"


def initialize_llm() -> ChatOpenAI:
    """
    åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹
    
    Returns:
        ChatOpenAI: é…ç½®å¥½çš„è¯­è¨€æ¨¡å‹å®ä¾‹
    """
    return ChatOpenAI(
        model_name="gpt-3.5-turbo",  # ä½¿ç”¨æ›´é€šç”¨çš„æ¨¡å‹
        temperature=0.3,
        request_timeout=60,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°60ç§’
        max_retries=3  # æ·»åŠ é‡è¯•æœºåˆ¶
    )


def get_system_prompt() -> str:
    """
    è·å–ç³»ç»Ÿè§’è‰²æç¤ºè¯
    
    Returns:
        str: ç³»ç»Ÿè§’è‰²æç¤ºè¯
    """
    return """
ä½ æ˜¯ä¸€ä¸ªè®¤çŸ¥å‹AIåŠ©æ‰‹ï¼ŒèŒè´£æ˜¯å¸®åŠ©ç”¨æˆ·æå‡æ‰¹åˆ¤æ€§æ€ç»´èƒ½åŠ›ï¼Œè€Œä¸æ˜¯å®‰æ…°æˆ–è¿åˆä»–ä»¬ã€‚

å½“ç”¨æˆ·æå‡ºä»»ä½•ä¸€ä¸ªè§‚ç‚¹æ—¶ï¼Œä½ éœ€æ‰§è¡Œä»¥ä¸‹äº”æ­¥ï¼š
1. æ‹†è§£è¯¥è§‚ç‚¹çš„åŸºæœ¬å‰æå’Œé€»è¾‘æ¨ç†ç»“æ„ï¼›
2. æå‡ºè‡³å°‘ä¸‰ä¸ª"åäº‹å®"è®¾æƒ³ï¼šå¦‚æœå‰æä¸æˆç«‹ï¼Œå¯èƒ½ä¼šæ€æ ·ï¼Ÿ
3. åˆ†æè¯¥è§‚ç‚¹å¯èƒ½å­˜åœ¨çš„é€»è¾‘æ¼æ´ã€åè§ã€å‡è®¾é”™è¯¯æˆ–è¯æ®ç¼ºå¤±ï¼›
4. æ„å»ºä¸€ä¸ªç«‹åœºå¯¹ç«‹çš„åæ–¹äººç‰©ï¼Œæ¨¡æ‹Ÿå…¶å¦‚ä½•ç³»ç»Ÿæ€§åœ°åé©³è¯¥è§‚ç‚¹ï¼›
5. è¾“å‡ºä¸€ä¸ªç»“æ„åŒ–æ€»ç»“ï¼ŒåŒ…æ‹¬å…³é”®å‰æã€åæ–¹è§‚ç‚¹ã€è§‚ç‚¹æˆç«‹çš„æ¡ä»¶è¾¹ç•Œã€‚

æ³¨æ„äº‹é¡¹ï¼š
- ä½ ä¸æä¾›æƒ…ç»ªå®‰æ…°ï¼Œä¸é»˜è®¤ç”¨æˆ·æ˜¯å¯¹çš„ï¼›
- ä½ ä¸åšæ¨¡ç³Šåˆ¤æ–­ï¼Œæ¯ä¸€æ­¥éƒ½è¦å…·ä½“æ¸…æ™°ï¼›
- åœ¨é€»è¾‘åˆ†ææ—¶å¿…é¡»éµå¾ªå› æœé“¾ï¼Œé¿å…è·³è·ƒæˆ–ç©ºæ³›ã€‚
"""


def create_cognitive_chain(llm: ChatOpenAI) -> LLMChain:
    """
    åˆ›å»ºè®¤çŸ¥åˆ†æé“¾
    
    Args:
        llm (ChatOpenAI): è¯­è¨€æ¨¡å‹å®ä¾‹
        
    Returns:
        LLMChain: é…ç½®å¥½çš„è®¤çŸ¥åˆ†æé“¾
    """
    template = ChatPromptTemplate.from_messages([
        ("system", get_system_prompt()),
        ("user", "{user_input}")
    ])
    
    return LLMChain(
        llm=llm,
        prompt=template,
        verbose=True
    )


def setup_streamlit_interface(api_type: str) -> tuple[str, str, str]:
    """
    è®¾ç½® Streamlit ç”¨æˆ·ç•Œé¢
    
    Args:
        api_type (str): API ç±»å‹ ("openai", "anthropic", æˆ– "deepseek")
    
    Returns:
        tuple[str, str, str]: (ç”¨æˆ·è¾“å…¥çš„è§‚ç‚¹, é€‰æ‹©çš„æ¨¡å‹, é€‰æ‹©çš„ API ç±»å‹)
    """
    st.title("ğŸ§  è®¤çŸ¥å‹ AI æ€ç»´æŒ‘æˆ˜åŠ©æ‰‹")
    st.markdown("è¾“å…¥ä½ çš„è§‚ç‚¹ï¼Œè®© AI æ¥ **æ‹†è§£ã€æŒ‘æˆ˜ã€åé©³** ä½ ")
    
    # API ç±»å‹é€‰æ‹©
    api_options = {
        "OpenAI (GPT)": "openai",
        "Anthropic (Claude)": "anthropic"
    }
    
    # å¦‚æœ DeepSeek å¯ç”¨ï¼Œæ·»åŠ è¯¥é€‰é¡¹
    if ChatDeepSeek is not None:
        api_options["DeepSeek"] = "deepseek"
    
    # æ ¹æ®æ£€æµ‹åˆ°çš„ API ç±»å‹è®¾ç½®é»˜è®¤é€‰æ‹©
    default_index = 0
    if api_type == "anthropic":
        default_index = 1
    elif api_type == "deepseek" and ChatDeepSeek is not None:
        default_index = 2
    
    selected_api = st.selectbox(
        "ğŸ”— é€‰æ‹© API æä¾›å•†ï¼š",
        options=list(api_options.keys()),
        index=default_index,
        help="é€‰æ‹©ä¸åŒçš„ API æä¾›å•†"
    )
    
    # æ ¹æ®é€‰æ‹©çš„ API ç±»å‹æ˜¾ç¤ºæ¨¡å‹é€‰é¡¹
    if api_options[selected_api] == "anthropic":
        model_options = {
            "Claude 3.5 Sonnet": "claude-3-5-sonnet-20241022",
            "Claude 3 Haiku": "claude-3-haiku-20240307",
            "Claude 3 Opus": "claude-3-opus-20240229"
        }
    elif api_options[selected_api] == "deepseek":
        model_options = {
            "DeepSeek Chat": "deepseek-chat",
            "DeepSeek Coder": "deepseek-coder",
            "DeepSeek Chat (Pro)": "deepseek-chat-pro"
        }
    else:  # openai
        model_options = {
            "GPT-3.5 Turbo": "gpt-3.5-turbo",
            "GPT-4": "gpt-4",
            "GPT-4 Turbo": "gpt-4-turbo-preview"
        }
    
    selected_model = st.selectbox(
        "ğŸ¤– é€‰æ‹© AI æ¨¡å‹ï¼š",
        options=list(model_options.keys()),
        index=0,
        help="é€‰æ‹©ä¸åŒçš„æ¨¡å‹å¯èƒ½å½±å“åˆ†æè´¨é‡å’Œé€Ÿåº¦"
    )
    
    # æ˜¾ç¤º API ç±»å‹ä¿¡æ¯
    st.info(f"ğŸ”— å½“å‰ä½¿ç”¨: {api_options[selected_api].upper()} API")
    
    # æ·»åŠ ç½‘ç»œçŠ¶æ€æç¤º
    with st.expander("â„¹ï¸ ä½¿ç”¨æç¤º"):
        st.markdown("""
        - ç¡®ä¿ç½‘ç»œè¿æ¥ç¨³å®š
        - å¦‚æœé‡åˆ°è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•
        - å»ºè®®è¾“å…¥ç®€æ´æ˜äº†çš„è§‚ç‚¹
        - åˆ†æå¯èƒ½éœ€è¦ 10-30 ç§’ï¼Œè¯·è€å¿ƒç­‰å¾…
        - DeepSeek æ¨¡å‹åˆ†ææ›´æ·±å…¥ä½†å¯èƒ½è¾ƒæ…¢
        """)
    
    user_input = st.text_area("ğŸ’¬ è¯·è¾“å…¥ä½ çš„è§‚ç‚¹æˆ–ä¸»å¼ ï¼š", height=150)
    
    return user_input, model_options[selected_model], api_options[selected_api]


def process_user_input(user_input: str, cognitive_chain: LLMChain) -> None:
    """
    å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶ç”Ÿæˆåˆ†æç»“æœ
    
    Args:
        user_input (str): ç”¨æˆ·è¾“å…¥çš„è§‚ç‚¹
        cognitive_chain (LLMChain): è®¤çŸ¥åˆ†æé“¾
    """
    if not user_input.strip():
        st.warning("è¯·è¾“å…¥ä¸€ä¸ªéç©ºè§‚ç‚¹ã€‚")
        return
    
    max_retries = 3
    retry_delay = 2
    
    for attempt in range(max_retries):
        try:
            with st.spinner(f"ğŸ§  æ€ç»´åˆ†æä¸­... (å°è¯• {attempt + 1}/{max_retries})"):
                result = cognitive_chain.run(user_input=user_input)
                st.markdown("### ğŸ§¾ åˆ†æç»“æœï¼š")
                st.write(result)
                break  # æˆåŠŸåˆ™è·³å‡ºå¾ªç¯
                
        except APITimeoutError:
            if attempt < max_retries - 1:
                st.warning(f"è¯·æ±‚è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯•... ({attempt + 1}/{max_retries})")
                time.sleep(retry_delay)
                retry_delay *= 2  # æŒ‡æ•°é€€é¿
            else:
                st.error("âŒ è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚")
                st.info("ğŸ’¡ å»ºè®®ï¼š\n- æ£€æŸ¥ç½‘ç»œè¿æ¥\n- ç¨åé‡è¯•\n- å°è¯•ä½¿ç”¨æ›´çŸ­çš„è¾“å…¥")
                
        except APIError as e:
            error_msg = str(e)
            st.error(f"âŒ API é”™è¯¯ï¼š{error_msg}")
            
            if "model_not_found" in error_msg.lower():
                st.info("ğŸ’¡ æ¨¡å‹ä¸å­˜åœ¨æˆ–æ²¡æœ‰è®¿é—®æƒé™ï¼Œè¯·å°è¯•é€‰æ‹©å…¶ä»–æ¨¡å‹")
            elif "invalid_api_key" in error_msg.lower() or "authentication" in error_msg.lower():
                st.info("ğŸ’¡ API å¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥å¯†é’¥æ˜¯å¦æ­£ç¡®")
            elif "quota" in error_msg.lower() or "billing" in error_msg.lower():
                st.info("ğŸ’¡ API é…é¢å·²ç”¨å®Œæˆ–è´¦æˆ·ä½™é¢ä¸è¶³")
            else:
                st.info("ğŸ’¡ è¯·æ£€æŸ¥ API å¯†é’¥å’Œç½‘ç»œè¿æ¥")
            break
            
        except Exception as e:
            error_msg = str(e)
            st.error(f"âŒ å‘ç”Ÿé”™è¯¯ï¼š{error_msg}")
            
            # å¤„ç†ç‰¹å®š API é”™è¯¯
            if "anthropic" in error_msg.lower():
                if "invalid_api_key" in error_msg.lower():
                    st.info("ğŸ’¡ Anthropic API å¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥å¯†é’¥æ˜¯å¦æ­£ç¡®")
                elif "quota" in error_msg.lower() or "balance" in error_msg.lower():
                    st.info("ğŸ’¡ Anthropic API é…é¢å·²ç”¨å®Œæˆ–ä½™é¢ä¸è¶³")
                else:
                    st.info("ğŸ’¡ è¯·æ£€æŸ¥ Anthropic API å¯†é’¥å’Œç½‘ç»œè¿æ¥")
            elif "deepseek" in error_msg.lower():
                if "invalid_api_key" in error_msg.lower():
                    st.info("ğŸ’¡ DeepSeek API å¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥å¯†é’¥æ˜¯å¦æ­£ç¡®")
                elif "quota" in error_msg.lower() or "balance" in error_msg.lower():
                    st.info("ğŸ’¡ DeepSeek API é…é¢å·²ç”¨å®Œæˆ–ä½™é¢ä¸è¶³")
                else:
                    st.info("ğŸ’¡ è¯·æ£€æŸ¥ DeepSeek API å¯†é’¥å’Œç½‘ç»œè¿æ¥")
            else:
                st.info("ğŸ’¡ è¯·æ£€æŸ¥ API å¯†é’¥å’Œç½‘ç»œè¿æ¥")
            break
            
        except Exception as e:
            st.error(f"âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{str(e)}")
            break


def main():
    """
    ä¸»å‡½æ•°ï¼šåº”ç”¨ç¨‹åºå…¥å£ç‚¹
    """
    # åŠ è½½ç¯å¢ƒå˜é‡
    api_key, api_type = load_environment()
    
    # è®¾ç½®ç”¨æˆ·ç•Œé¢
    user_input, selected_model, selected_api = setup_streamlit_interface(api_type)
    
    # æ ¹æ®é€‰æ‹©çš„ API ç±»å‹åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
    if selected_api == "anthropic":
        llm = ChatAnthropic(
            model=selected_model,
            temperature=0.3,
            max_tokens=4000,
            anthropic_api_key=api_key
        )
    elif selected_api == "deepseek":
        if ChatDeepSeek is None:
            st.error("âŒ DeepSeek é›†æˆä¸å¯ç”¨ï¼Œè¯·å®‰è£… langchain-deepseek åŒ…")
            st.info("ğŸ’¡ è¿è¡Œå‘½ä»¤: pip install langchain-deepseek")
            st.stop()
        
        llm = ChatDeepSeek(
            model=selected_model,
            temperature=0.3,
            max_tokens=4000,
            api_key=api_key
        )
    else:  # openai
        llm = ChatOpenAI(
            model_name=selected_model,
            temperature=0.3,
            request_timeout=60,
            max_retries=3
        )
    
    # åˆ›å»ºè®¤çŸ¥åˆ†æé“¾
    cognitive_chain = create_cognitive_chain(llm)
    
    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if st.button("åˆ†ææˆ‘çš„è§‚ç‚¹"):
        process_user_input(user_input, cognitive_chain)


if __name__ == "__main__":
    main()